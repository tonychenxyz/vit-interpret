{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib.pyplot import text\n",
    "from dataloader.waterbird import Waterbird, WB_DomainTest\n",
    "import clip\n",
    "import torch\n",
    "import clipfolder.clip as clipours\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.transform import resize \n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#set up the device as 2nd GPU\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transofmer total layers 12\n",
      " mask for layer 0\n",
      " mask for layer 1\n",
      " mask for layer 2\n",
      " mask for layer 3\n",
      " mask for layer 4\n",
      " mask for layer 5\n",
      " mask for layer 6\n",
      " mask for layer 7\n",
      " mask for layer 8\n",
      " mask for layer 9\n",
      " mask for layer 10\n",
      " mask for layer 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transofmer total layers 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transofmer total layers 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transofmer total layers 12\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "for which_layer in [0,12]:#range(0, 13):\n",
    "    model, preprocess = clipours.load('ViT-B/32', device, jit=False, extract_last_k_th_token=12 - which_layer) # model are ranked from the early layers to last layers interpretation.\n",
    "    model_list.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_attention_attn_blocks = [i for i in model_list[0].visual.transformer.resblocks.children()]\n",
    "attn_blocks = [i for i in model_list[-1].visual.transformer.resblocks.children()]\n",
    "logit_scale = model_list[-1].logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12286/12286 [00:00<00:00, 55758.43it/s]\n",
      "/tmp/ipykernel_33395/2628356709.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  polygon_arr = np.array(annotation['instance_polygon'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from visual_genome import api\n",
    "import requests\n",
    "from PIL import Image as PIL_Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('../mic/vaw_dataset/data/val.json') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "text = {\"\"}\n",
    "for i in tqdm(range(len(annotations))):\n",
    "    text = text.union(annotations[i]['positive_attributes'])\n",
    "    text = text.union(annotations[i]['negative_attributes'])\n",
    "    # text = text.union([annotations[i]['object_name']])\n",
    "text = list(text)\n",
    "\n",
    "\n",
    "agg_dict = {}\n",
    "\n",
    "skip_return = [None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "for annotation in annotations:\n",
    "    try:\n",
    "        polygon_arr = np.array(annotation['instance_polygon'])\n",
    "        min_x = int(polygon_arr[0, :, 0].min())\n",
    "        max_x = int(polygon_arr[0, :, 0].max())\n",
    "        min_y = int(polygon_arr[0, :, 1].min())\n",
    "        max_y = int(polygon_arr[0, :, 1].max())\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if annotation['image_id'] not in agg_dict:\n",
    "        agg_dict[annotation['image_id']] = {\"positive_attributes\": [], \"object_names\": [], \"bboxes\": []}\n",
    "\n",
    "    agg_dict[annotation['image_id']]['positive_attributes'] = list(set(agg_dict[annotation['image_id']]['positive_attributes']).union(annotation[\"positive_attributes\"]))\n",
    "    agg_dict[annotation['image_id']]['object_names'] = list(set(agg_dict[annotation['image_id']]['object_names']).union([annotation[\"object_name\"]]))\n",
    "    agg_dict[annotation['image_id']]['bboxes'].append([min_x, min_y, max_x, max_y])\n",
    "agg_dict.pop(\"2364650\", None)\n",
    "img_ids = [i for i in agg_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interpretation_from_layer(x, layer, attn_blocks, ln_post, text_features, logit_scale, ans_with_cat, smooth_n = 500, center_l2s = None, get_top = True):\n",
    "    with torch.no_grad():\n",
    "        if layer <= len(attn_blocks) - 1:\n",
    "            x = attn_blocks[layer](x)\n",
    "\n",
    "        for i in range(layer+1,len(attn_blocks)):\n",
    "            random_xs = []\n",
    "            if smooth_n >= 1:\n",
    "                for random_idx in range(smooth_n):\n",
    "                    epsilon = torch.randn_like(x) * center_l2s[i-1]\n",
    "                    x_random = x + epsilon / 20\n",
    "                    x_random = attn_blocks[i](x_random)\n",
    "                    random_xs.append(x_random)\n",
    "                x = torch.stack(random_xs, dim=0).mean(dim=0)\n",
    "            else:\n",
    "                x = attn_blocks[i](x)\n",
    "        \n",
    "        random_xs = []\n",
    "        if smooth_n >= 1 and layer <= len(attn_blocks) - 1:\n",
    "            for random_idx in range(smooth_n):\n",
    "                epsilon = torch.randn_like(x) * center_l2s[-1]\n",
    "                x_random = x + epsilon\n",
    "                x_random = x_random.permute(1, 0, 2)\n",
    "                x_random = torch.cat([ln_post(x_random[:, idx, :]).unsqueeze(1) for idx in range(x_random.size(1))], dim=1)\n",
    "                image_features = x_random @ model.visual.proj\n",
    "                random_xs.append(image_features)\n",
    "            image_features = torch.stack(random_xs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            image_features = x.permute(1, 0, 2)\n",
    "            image_features = torch.cat([ln_post(image_features[:, idx, :]).unsqueeze(1) for idx in range(image_features.size(1))], dim=1)\n",
    "            image_features = image_features @ model.visual.proj\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "        logits_img = (logit_scale * image_features) @ text_features.t()\n",
    "        \n",
    "        if get_top:\n",
    "            pseudo_target = np.argmax(logits_img.cpu().numpy(), axis=2)\n",
    "            ans_2 = []\n",
    "            for bs in range(pseudo_target.shape[0]):\n",
    "                ans_3 = []\n",
    "                for it in range(0, pseudo_target.shape[1]): # for each token, put 1 here to avoid CLS token replace\n",
    "                    index = pseudo_target[bs, it]\n",
    "                    ans_3.append(ans_with_cat[index])\n",
    "                ans_2.append(ans_3)\n",
    "\n",
    "            return ans_2\n",
    "        else:\n",
    "            ans_with_cat = np.array(ans_with_cat)\n",
    "            psuedo_target = np.argsort(logits_img.cpu().numpy(), axis=2)\n",
    "            psuedo_target = psuedo_target[:, :, ::-1]\n",
    "            ans_2 = []\n",
    "            for bs in range(psuedo_target.shape[0]):\n",
    "                ans_3 = []\n",
    "                for it in range(0, psuedo_target.shape[1]):\n",
    "                    index = psuedo_target[bs, it]\n",
    "                    ans_3.append(ans_with_cat[index])\n",
    "                ans_2.append(ans_3)\n",
    "            \n",
    "            return ans_2\n",
    "        \n",
    "def get_interpretation_all_layers(x, attn_blocks, no_attention_attn_blocks, ln_post, text_features, logit_scale, ans_with_cat, smooth_n = 500, center_l2s = None, get_top = True):\n",
    "    ans = []\n",
    "    with torch.no_grad():\n",
    "        x = model.visual.conv1(x.type(torch.cuda.HalfTensor))\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]   [B, channel, H*W]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]       [B, H*W, channel]\n",
    "        x = torch.cat([model.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + model.visual.positional_embedding.to(x.dtype)\n",
    "        x = model.visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND    [H*W, B, channel]\n",
    "\n",
    "        for i in range(len(attn_blocks)):\n",
    "            interpretations = get_interpretation_from_layer(x, i, no_attention_attn_blocks, ln_post, text_features, logit_scale, ans_with_cat, smooth_n = smooth_n, center_l2s = center_l2s, get_top = get_top)\n",
    "            ans.append(interpretations)\n",
    "            x = attn_blocks[i](x)\n",
    "        interpretations = get_interpretation_from_layer(x, i+1, no_attention_attn_blocks, ln_post, text_features, logit_scale, ans_with_cat, smooth_n = smooth_n, center_l2s = center_l2s, get_top = get_top)\n",
    "        ans.append(interpretations)\n",
    "        \n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "class VAWDataset(Dataset):\n",
    "    def __init__(self, img_ids, root_dir, \n",
    "                 transform=None):\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_ids = img_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = Image.open(self.root_dir + '/' + img_id + '.jpg').convert(\"RGB\") # you can use PIL to open the image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "    \n",
    "train_dataset = VAWDataset(list(agg_dict.keys()), root_dir='vaw_images', transform=preprocess)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 256, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing as mp\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class VAWVerificationDataset(Dataset):\n",
    "    def __init__(self, img_ids, root_dir, \n",
    "                 transform=None, random_n = 5):\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_ids = img_ids\n",
    "        self.random_n = random_n\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = Image.open(self.root_dir + '/' + img_id + '.jpg').convert(\"RGB\") # you can use PIL to open the image\n",
    "        images = []\n",
    "\n",
    "        original_image = np.array(img)\n",
    "        original_image = Image.fromarray(original_image)\n",
    "        images.append(original_image)\n",
    "        masked_image = np.array(img)\n",
    "        for bbox in agg_dict[img_id]['bboxes']:\n",
    "            min_x, min_y, max_x, max_y = bbox\n",
    "            try:\n",
    "                masked_image[min_y:max_y, min_x:max_x, :] = 0\n",
    "            except:\n",
    "                print(\"error\", img_id)\n",
    "        \n",
    "        masked_image = Image.fromarray(masked_image)\n",
    "        images.append(masked_image)\n",
    "\n",
    "        for i in range(self.random_n):\n",
    "            masked_image = np.array(img)\n",
    "\n",
    "            total_box_area = 0\n",
    "            for bbox in agg_dict[img_id][\"bboxes\"]:\n",
    "                min_x, min_y, max_x, max_y = bbox\n",
    "                box_width = max_x - min_x\n",
    "                box_height = max_y - min_y\n",
    "                box_area = box_width * box_height\n",
    "                total_box_area += box_area\n",
    "                total_box_area_ratio = total_box_area / (masked_image.shape[0] * masked_image.shape[1])\n",
    "\n",
    "                mask = np.random.rand(int(masked_image.shape[0]/50), int(masked_image.shape[1]/50))\n",
    "                mask = mask < (total_box_area_ratio / (1-total_box_area_ratio))\n",
    "                mask = resize(mask,masked_image.shape)\n",
    "\n",
    "            for bbox in agg_dict[img_id][\"bboxes\"]:\n",
    "                min_x, min_y, max_x, max_y = bbox\n",
    "                mask[min_y:max_y, min_x:max_x, :] = False\n",
    "\n",
    "            masked_image[mask] = 0\n",
    "            \n",
    "            masked_image = Image.fromarray(masked_image)\n",
    "            images.append(masked_image)        \n",
    "        \n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images ]\n",
    "        \n",
    "        object_names = agg_dict[img_id][\"object_names\"]\n",
    "\n",
    "        # print(object_names)\n",
    "        return \",\".join(object_names), *images\n",
    "    \n",
    "verification_dataset = VAWVerificationDataset(list(agg_dict.keys()), random_n = 2, root_dir='vaw_images', transform=preprocess)\n",
    "verification_loader = torch.utils.data.DataLoader(verification_dataset, batch_size = 40, shuffle=False, num_workers=mp.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_l2s_batches = []\n",
    "with_attention_center_batches = []\n",
    "no_attention_center_batches = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        center_l2s = []\n",
    "        with_attention_center_layers = []\n",
    "        no_attention_center_layers = []\n",
    "        X = batch\n",
    "        X = X.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = model.visual.conv1(X.type(torch.cuda.HalfTensor))\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]   [B, channel, H*W]\n",
    "            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]       [B, H*W, channel]\n",
    "            x = torch.cat([model.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "            x = x + model.visual.positional_embedding.to(x.dtype)\n",
    "            x = model.visual.ln_pre(x)\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND    [H*W, B, channel]\n",
    "\n",
    "            for i in range(len(attn_blocks)):\n",
    "                x = attn_blocks[i](x)\n",
    "                x_no_attention = no_attention_attn_blocks[i](x)\n",
    "                with_attention_center = x.mean(axis=1)\n",
    "                no_attention_center = x_no_attention.mean(axis=1)\n",
    "                with_attention_center_layers.append(with_attention_center)\n",
    "                no_attention_center_layers.append(no_attention_center)\n",
    "        with_attention_center_batches.append(with_attention_center_layers)\n",
    "        no_attention_center_batches.append(no_attention_center_layers)\n",
    "\n",
    "center_l2s = []\n",
    "for layer in range(12):\n",
    "    with_attention_center_this_layer = [with_attention_center_batches[i][layer].unsqueeze(1) for i in range(len(with_attention_center_batches))]\n",
    "    with_attention_center_this_layer = torch.cat(with_attention_center_this_layer, dim=1)\n",
    "    with_attention_center = with_attention_center_this_layer.mean(dim=1)\n",
    "    no_attention_center_this_layer = [no_attention_center_batches[i][layer].unsqueeze(1) for i in range(len(no_attention_center_batches))]\n",
    "    no_attention_center_this_layer = torch.cat(no_attention_center_this_layer, dim=1)    \n",
    "    no_attention_center = no_attention_center_this_layer.mean(dim=1)\n",
    "\n",
    "    center_l2 = torch.sqrt((with_attention_center - no_attention_center)**2).unsqueeze(1)    \n",
    "    center_l2s.append(center_l2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [1:02:00<00:00, 44.82s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    rank_change_dataset = []\n",
    "    for batch in tqdm(verification_loader):\n",
    "        object_names_raw = batch[0]\n",
    "        object_names = []\n",
    "        for object_name_raw in object_names_raw:\n",
    "            object_names += object_name_raw.split(\",\")\n",
    "        \n",
    "        all_text = text + object_names\n",
    "        ans_with_cat = all_text\n",
    "        all_prompts = [f\"this can be described as \" + each for each in all_text]\n",
    "        text_tokens = clip.tokenize(all_prompts).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_fea, text_fea = model_list[-1](batch[1].to(device), text_tokens, get_all_last=True)\n",
    "            text_fea = text_fea / text_fea.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        interpretations_all_type = []\n",
    "        for image_type_idx in range(1, len(batch)):\n",
    "            # type_interpretations = get_interpretation_all_layers(batch[image_type_idx].to(device), attn_blocks, no_attention_attn_blocks, model_list[-1].visual.ln_post, text_fea, logit_scale, ans_with_cat, smooth_n = 0, center_l2s = None, get_top = False)\n",
    "            type_interpretations = get_interpretation_all_layers(batch[image_type_idx].to(device), attn_blocks, no_attention_attn_blocks, model_list[-1].visual.ln_post, text_fea, logit_scale, ans_with_cat, smooth_n = 100, center_l2s = center_l2s, get_top = False)\n",
    "            interpretations_all_type.append(type_interpretations)\n",
    "\n",
    "        \n",
    "        for image_i in range(len(interpretations_all_type[0][0])):\n",
    "            original_top_interpretations = [[interpretations_all_type[0][i][image_i][j][0] for j in range(50)] for i in range(13)]\n",
    "            rank_change_image = []\n",
    "            for image_type_i in range(1,len(interpretations_all_type)):\n",
    "                rank_change_image_type = []\n",
    "                for i in range(13):\n",
    "                    rank_change_layer = []\n",
    "                    for j in range(50):\n",
    "                        rank_change_layer.append(np.where(interpretations_all_type[image_type_i][i][image_i][j] == original_top_interpretations[i][j])[0][0])\n",
    "                    rank_change_image_type.append(rank_change_layer)\n",
    "                rank_change_image.append(rank_change_image_type)        \n",
    "            rank_change_dataset.append(rank_change_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_change_dataset = np.array(rank_change_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 12, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_l2s_arr = torch.cat(center_l2s, dim = 1).cpu().numpy()\n",
    "center_l2s_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAW\n",
      "CLS drift 0.0791015625\n",
      "other tokens mean drift 0.0870361328125\n"
     ]
    }
   ],
   "source": [
    "print(\"VAW\")\n",
    "print(f\"CLS drift {center_l2s_arr.mean(axis = 1).mean(axis = 1)[0]}\")\n",
    "print(f\"other tokens mean drift {center_l2s_arr.mean(axis = 1).mean(axis = 1)[1:].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2732 , 0.0604 , 0.05307, 0.04626, 0.03717, 0.0409 , 0.03064,\n",
       "       0.0369 , 0.04086, 0.0758 , 0.1335 , 0.2135 ], dtype=float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_l2s_arr.mean(axis = 0).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"overall vaw mask rank change {rank_change_dataset[:,0,:,:].mean()}, random mask rank change {rank_change_dataset[:,1:,:,:].mean()}\")\n",
    "\n",
    "for layer_idx in range(13):\n",
    "    print(f\"vaw mask rank change layer {layer_idx} {rank_change_dataset[:,0,layer_idx,:].mean()}, random mask rank change layer {layer_idx} {rank_change_dataset[:,1:,layer_idx,:].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vanilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
